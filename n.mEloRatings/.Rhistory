library(devtools)
library(devtools)
library(data.table)
library(DescTools)
library(ggplot2)
library(matrixStats)
library(repr)
library(ggrepel)
library(stringr)
library(dplyr)
# setting plot sizes
options(repr.plot.width=24, repr.plot.height=8)
data_repo = '/Users/Jenny/OneDrive - University of Surrey/Documents/PhD/3_Preparedness_questionnaire/Code_and_output/Data_analysis_v1/Data_repository'
data_repo = '/Users/Jenny/OneDrive - University of Surrey/Documents/PhD/3_Preparedness_questionnaire/Code_and_output/Data_analysis_v1/Data_repository'
# producing path to the qualtrics output files
qualtrics_repo_path = file.path(data_repo, 'Qualtrics')
list_qualtrics = list.files(path = qualtrics_repo_path)
list_qualtrics
df_qualtrics_wide = read.csv(file.path(qualtrics_repo_path, list_qualtrics[7]))
# getting the total number of rows in the wide format
num_rows_wide = dim(df_qualtrics_wide)[1]
# filtering out the first two rows
df_qualtrics_wide = df_qualtrics_wide[3:num_rows_wide,]
# and only including FINISHED surveys
df_qualtrics_wide = df_qualtrics_wide[as.numeric(df_qualtrics_wide$Finished) == 1,]
# and producing a metadata dataframe for FINISHED surveys
# metadata columns
list_meta_cols = c('Duration..in.seconds.','Finished','ResponseId')
# metadata dataframe
df_qualtrics_meta = df_qualtrics_wide[,list_meta_cols]
head(df_qualtrics_meta)
df_qualtrics = reshape2::melt(data = df_qualtrics_wide, id.vars = c('StartDate','EndDate','Status','IPAddress','Progress','Duration..in.seconds.','Finished','RecordedDate','ResponseId','RecipientLastName','RecipientFirstName','RecipientEmail','ExternalReference','LocationLatitude','LocationLongitude','DistributionChannel','UserLanguage','Q_RecaptchaScore','Q110'), variable.name = 'Q', value.name = 'A')
df_qualtrics = df_qualtrics[order(df_qualtrics$ResponseId,df_qualtrics$Q),c('ResponseId','Q110','Q','A')]
q_classification = function(x){
if (substr(toString(x), 1, 1) == 'X'){
return ('Pairwise')
}
if (substr(toString(x), 1, 4) == 'Q169_'){
return ('Likert')
}
else {
return ('Initial')
}
}
df_qualtrics$Q_classification = lapply(unique(df_qualtrics$Q), q_classification)
df_qualtrics_pw = df_qualtrics[df_qualtrics$Q_classification == 'Pairwise',c('ResponseId','Q','A')]
# and we're only interested in the pairwise Qs that people answered (as there are 4095 possible Qs)
df_qualtrics_pw = df_qualtrics_pw[df_qualtrics_pw$A != '',]
# and now casting the A's to numeric types
df_qualtrics_pw$A = as.numeric(df_qualtrics_pw$A)
head(df_qualtrics_pw)
binary_coding = function(x, response){
if (x == response){
return (1)
}
else {
return (0)
}
}
df_qualtrics_pw_average = df_qualtrics_pw
df_qualtrics_pw_average$Response.1 = as.numeric(lapply(df_qualtrics_pw$A, FUN=binary_coding, response=1))
df_qualtrics_pw_average$Response.2 = as.numeric(lapply(df_qualtrics_pw$A, FUN=binary_coding, response=2))
df_qualtrics_pw_average$Response.DontUnderstand = as.numeric(lapply(df_qualtrics_pw$A, FUN=binary_coding, response=3))
head(df_qualtrics_pw_average)
df_agg = aggregate(x = df_qualtrics_pw_average[,c('Response.1','Response.2','Response.DontUnderstand')], by = list(df_qualtrics_pw_average$ResponseId), FUN='mean')
head(df_agg)
#each data point on the chart is a single respondent
df_agg_melt = reshape2::melt(df_agg, id.vars = 'Group.1', variable.name = 'Response', value.name = 'Average')
p <- ggplot(df_agg_melt, aes(x=Response, y=Average, color=Response)) + geom_boxplot(outlier.shape = NA) +
geom_jitter(shape=2, size=2, position=position_jitter(0.2)) +
ylim(c(-0.01,1)) +
labs(x='Response', y = "Proportion of responses selected by each participant", title='Proportion of instances each response is selected by each participant')
p
ggsave(p, file='/Users/Jenny/OneDrive - University of Surrey/Documents/PhD/3_Preparedness_questionnaire/Code_and_output/Data_analysis_v1/Bristol_supervisors/Metadata/responseproportions_Bristolclinicalsupervisors.png')
# calculating frequencies of questions (they'll be our denominator)
df_denom = aggregate(x = df_qualtrics_pw[,c('ResponseId')], by=list(df_qualtrics_pw$Q), FUN='length')
# renaming column names
colnames(df_denom) = c('Q','total')
head(df_denom)
# calculating numerator: the number of times a particular answer (1,2,3) has been chosen for a given question
df_freq = aggregate(x = df_qualtrics_pw[,c('ResponseId')], by=list(df_qualtrics_pw$Q,df_qualtrics_pw$A), FUN='length')
# renaming columns again
colnames(df_freq) = c('Q','A','freq')
# ordering by Q and answer
df_freq = df_freq[order(df_freq$Q,df_freq$A),]
head(df_freq)
# merging numerator and denominator
df_consensus = merge(x = df_freq, y = df_denom, by = 'Q', all = TRUE)
# producing consensus rate
df_consensus$consensus_rate = df_consensus$freq / df_consensus$total
# applying consensus of >=50% and more than one response per question
#consensus.rate = where a question has been answered more than once, it is the proportion of instances where the mode response was given. e.g., if a question was asked twice, both times the response was A then consensus.rate = 1.0, if a question was asked 3 times, and twice the response was B, then consensus.rate = 0.66667.
df_consensus = df_consensus[(df_consensus$consensus_rate >=0.5) & (df_consensus$total >1),]
head(df_consensus)
# average consensus rate
mean(df_consensus$consensus_rate)
# producing path to the reference files
reference_repo_path = file.path(data_repo, 'Reference')
# regular expression for reference files
reference_list_pattern = 'question_reference_group_v\\d+_\\d+.csv'
# regular expression for the characteristic list
prep_char_list_pattern = 'Preparedness_characteristic_list_v\\d+.csv'
# producing lists of the reference and preparedness characteristics files
list_reference = list.files(reference_repo_path, pattern = reference_list_pattern)
file_prepare = list.files(reference_repo_path, pattern = prep_char_list_pattern)
list_reference
df_reference_list = list()
for (ref_idx in 1:length(list_reference)){
# producing file path for each of the 8 reference files
reference_file_path = file.path(reference_repo_path, list_reference[ref_idx])
# loading each reference file as its own data frame
df_ref = read.csv(reference_file_path)
# appending data frame to a list
df_reference_list[[ref_idx]] = df_ref
}
# and now combining those reference data frames together, by binding the rows (essentially performing a union), into one big data frame
df_reference = do.call('rbind', df_reference_list)
# overall dimensions of the reference data frames as you'd expect
dim(df_reference)
head(df_reference)
df_prepare = read.csv(file = file.path(reference_repo_path, file_prepare[1]))
head(df_prepare)
question_string = function(x){
q_string = toString(x)
return (paste('X',q_string, sep=''))
}
# producing new data frame that's narrower than the full reference data frame
df_link = df_reference[c('QuestionId','A','A.Code','B','B.Code')]
# transforming QuestionId column into Q column
df_link$Q = factor(as.character(lapply(X = df_link$QuestionId, FUN = question_string)))
head(df_link)
head(df_qualtrics_pw)
# merging the reference data onto the pairwise output
df_pw = merge(x = df_qualtrics_pw, y = df_link[,c('Q','A.Code','B.Code')], by = 'Q', all.x = TRUE)
dim(df_pw)
#Find rows which contain A==3 (i.e., I don't know)
answer_is_3 <- df_pw %>% filter(df_pw$A==3)
answer_is_3
#Remove these rows from the data frame for running the Elo algorithm
df_pw <- df_pw %>% filter(df_pw$A != 3)
dim(df_pw)
head(df_pw)
win = function(ans, a, b){
if (ans == 1){
return (a)
}
else {
return (b)
}
}
lose = function(ans, a, b){
if (ans == 2){
return (a)
}
else {
return (b)
}
}
df_pw$Winner = mapply(FUN = win, ans = df_pw$A, a = df_pw$A.Code, b = df_pw$B.Code)
df_pw$Loser = mapply(FUN = lose, ans = df_pw$A, a = df_pw$A.Code, b = df_pw$B.Code)
head(df_pw[order(df_pw$Q),], 10)
write.csv(df_pw, 'Winnerloserdata_Bristolclinicalsupervisors.csv')
# initial conditions
initial_rating = 0
k = 100
initialise_scores = function(df_prepare, code_type, initial_rating){
# code list: we'll assign initial ratings to each code
code_list = df_prepare[code_type][[1]]
# the names of the list of ratings are the abbreviated code names
scores = vector(mode="list", length=length(code_list))
names(scores) = code_list
# assigning initial ratings to codes
for (i in 1:length(code_list)){
scores[[i]] = initial_rating
}
return (scores)
}
expected_win = function(r1, r2){
return (1.0 / (1 + 10^( (r2-r1) / 400) ))
}
elo_update = function(input_rating_winner, input_rating_loser, k){
# calculating probabilities of winner and loser (pre-comparison)
P_winner = expected_win(input_rating_winner, input_rating_loser)
P_loser = expected_win(input_rating_loser, input_rating_winner)
# updating the ratings
output_rating_winner = input_rating_winner + (k * (1 - P_winner))
output_rating_loser = input_rating_loser + (k * (0 - P_loser))
# outputting the updated winner and loser ratings
return (c(output_rating_winner, output_rating_loser))
}
elo_sequence = function(df_pw, df_prepare, code_type, initial_rating, k){
# initialising scores
scores = initialise_scores(df_prepare, code_type, initial_rating)
# getting the number of rows
num_pw = dim(df_pw)[1]
# randomising the order of the pairwise comparision dataframe
#set.seed(123)
df_pw_rand = df_pw[sample(num_pw),c('ResponseId','Q','Winner','Loser')]
# iterating through the pairwise comparisons
for (i in 1:num_pw){
# getting winner and loser codes for each row
winner = df_pw_rand[i,'Winner']
loser = df_pw_rand[i,'Loser']
# getting ratings
winner_rating = scores[winner][[1]]
loser_rating = scores[loser][[1]]
# performing the match (using the function we defined above)
match = elo_update(winner_rating, loser_rating, k)
# updating our scores for the winner and loser
scores[winner] = match[1]
scores[loser] = match[2]
}
return (scores)
}
melo = function(melo_iterations, df_pw, df_prepare, code_type, initial_rating, k){
# initialising a list of elo dataframes
elo_list = 0
# iterating through the mean elo (mElo) iterations
for (i in 1:melo_iterations){
# producing a single iteration of the elo scores
elo_iteration = elo_sequence(df_pw, df_prepare, code_type, initial_rating, k)
# appending the elo scores to elo list
elo_list[i] = data.frame(unlist(elo_iteration))
}
# binding each elo iteration together
df_elo = data.frame(do.call('rbind', elo_list))
colnames(df_elo) = df_prepare[code_type][[1]]
# calculating mElo by taking the mean of each column
df_melo = data.frame(apply(X = df_elo, MARGIN = 2, FUN = mean))
df_melo$Code = rownames(df_melo)
colnames(df_melo) = c('mElo','Code')
# merging the melo scores back on the preparation code reference data
df = merge(df_melo, df_prepare, by.x = 'Code', by.y = code_type, all.x = TRUE)
# and finally ordering by melo rating
df = df[order(-df$mElo),]
return (df)
}
melo_iterations = 500
df_melo_results = melo(melo_iterations, df_pw, df_prepare, 'Preparedness_characteristic_code', initial_rating, k)
#Creating the items we need for the min-max normalisation calculation: X_new = (X-Xmin)/(Xmax-Xmin)
mElo_max <- max(df_melo_results$mElo)
mElo_max
mElo_min <- min(df_melo_results$mElo)
mElo_min
Denom <- mElo_max-mElo_min
Denom
#Creating a vector of mElo ratings
mElo_ratings <- df_melo_results$mElo
#Creating an empty vector for normalised mElo results
normalised_mElo_ratings <- 0
for(i in 1:91){
normalised_mElo_ratings[i] <- (mElo_ratings[i]-mElo_min)/Denom
print(normalised_mElo_ratings[i])
}
#Adding the normalised results to the dataframe
df_melo_results <- cbind(df_melo_results, new_col = normalised_mElo_ratings)
colnames(df_melo_results)[colnames(df_melo_results)=="new_col"] <- "normalised_mElo_rating"
#Removing the row names
rownames(df_melo_results) <- NULL
head(df_melo_results)
write.csv(df_melo_results,'/Users/Jenny/OneDrive - University of Surrey/Documents/PhD/3_Preparedness_questionnaire/Code_and_output/Data_analysis_v1/Bristol_supervisors/Minmax_scaled_mElo/Minmax_mEloratings_Bristolclinicalsupervisors.csv')
use_r('initialise_scores')
use_r('expected_win')
use_r('elo_update')
use_r('elo_sequence')
use_r('mElo')
use_r('n.Melo')
use_r('norm_Melo')
library(n.mEloRatings)
library(n.mEloRatings)
use_r('test')
n.mEloRatings::test(2)
n.mEloRatings::test(2)
library(n.mEloRatings)
n.mEloRatings::test(2)
test(2)
devtools::load_all(".")
test(2)
test(14)
devtools::load_all(".")
library(n.mEloRatings)
use_r('Elo')
devtools::load_all(".")
library(n.mEloRatings)
devtools::load_all(".")
test(17)
use_r('Loader')
p = '/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/Example_data_set.csv'
r = '/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/'
f = 'Example_data_set.csv'
file.path(r,f)
data.frame(file.path(r,f))
read.csv(file.path(r,f))
devtools::load_all(".")
loader('/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/','Example_data_set.csv')
loader('/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/','example_dataset.csv')
df_pw = loader('/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/','example_dataset.csv')
df_pw
library(n.mEloRatings)
devtools::load_all(".")
devtools::load_all(".")
library(n.mEloRatings)
test(20)
devtools::load_all(".")
f <- '/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/example_dataset.csv'
df_pw <- loader(f)
df_ow
df_pw
df_pw
df_pw$A.Code
unique(df_pw$A.Code)
type(df_pw$A.Code)
typeof(df_pw$A.Code)
c(unique(df_pw$A.Code), unique(df_pw$B.Code))
unique(c(unique(df_pw$A.Code), unique(df_pw$B.Code)))
unique(c(unique(df_pw$A.Code), unique(df_pw$B.Code)))[2]
unique(c(unique(df_pw$A.Code), unique(df_pw$B.Code)))[1]
devtools::load_all(".")
df_pw
initialise_scores(df_pw)
scores = initialise_scores(df_pw)
scores
scores$Melon
devtools::load_all(".")
elo_sequence(df_pw)
df_pw
head(df_pw)
devtools::load_all(".")
elo_sequence(df_pw)
data.frame(elo_sequence(df_pw))
data.frame(unlist(elo_sequence(df_pw)))
devtools::load_all(".")
data.frame(unlist(elo_sequence(df_pw)))
library(n.mEloRatings)
devtools::load_all(".")
f <- '/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/example_dataset.csv'
df_pw <- loader(f)
elo_sequence(df_pw)
unlist(elo_sequence(df_pw))
data.frame(unlist(elo_sequence(df_pw)))
df_elo <- data.frame(unlist(elo_sequence(df_pw)))
df_elo
df_elo[,1]
df_elo[,2]
df_elo[,0]
dim(df_elo)
df_elo
colnames(df_elo)
colnames(df_elo)
rownames(df_elo)
elo_list <- 0
elo_list[1] <- df_elo
elo_list[2] <- df_elo
df_elo_concat <- data.frame(do.call('rbind', elo_list))
df_elo_concat
colnames(df_elo) = rownames(df_elo)
df_melo = data.frame(apply(X = df_elo_concat, MARGIN = 2, FUN = mean))
df_melo
df_melo$Code = rownames(df_elo)
df_melo
colnames(df_melo) = c('mElo','Code')
df_melo
scores
unlist(scores)
df_melo
df_elo_concat
colnames(df_elo_concat)
rownames(df_elo)
unlist(rownames(df_elo))
colnames(df_elo_concat) = rownames(df_elo)
df_elo_concat
df_melo = data.frame(apply(X = df_elo_concat, MARGIN = 2, FUN = mean))
df_melo
devtools::load_all(".")
melo(df_pw)
melo(df_pw, melo_iterations = 10)
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
elo_iteration
elo_sequence(df_pw)
unlist(elo_sequence(df_pw))
rownames(unlist(elo_sequence(df_pw)))
rownames(elo_sequence(df_pw))
elo_sequence(df_pw)
colnames(elo_sequence(df_pw))
colnames(elo_sequence(df_pw)[[1]])
rownames(elo_sequence(df_pw)[[1]])
rownames(elo_sequence(df_pw))
rownames(unlist(elo_sequence(df_pw)))
rownames(data.frame(unlist(elo_sequence(df_pw))))
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
melo(df_pw, melo_iterations = 10)[1]
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)[1]
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
devtools::load_all(".")
melo(df_pw, melo_iterations = 10)
melo(df_pw, melo_iterations = 20)
melo(df_pw, melo_iterations = 50)
df_melo <- melo(df_pw, melo_iterations = 20)
df_melo
df_melo$n.mElo <- n.mElo.ratings(df_melo$mElo)
df_melo
devtools::load_all(".")
df_melo$n.mElo <- n.mElo.ratings(df_melo$mElo)
df_melo
devtools::load_all(".")
df_pw = loader('/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/example_dataset.csv')
df_pw
initialise_scores(df_pw)
unlist(initialise_scores(df_pw))
data.frame(unlist(initialise_scores(df_pw)))
elo_sequence(df_pw)
elo_sequence(df_pw, initial_rating=400, k=200)
elo_sequence(df_pw)
melo(df_pw, melo_iterations = 20)
elo_sequence(df_pw)
data.frame(unlist(elo_sequence(df_pw)))
devtools::load_all(".")
elo_sequence(df_pw)
devtools::load_all(".")
elo_sequence(df_pw)
devtools::load_all(".")
elo_sequence(df_pw)
melo(df_pw, melo_iterations = 20)
devtools::load_all(".")
melo(df_pw, melo_iterations = 20)
elo_sequence(df_pw)
devtools::load_all(".")
elo_sequence(df_pw)
melo(df_pw, melo_iterations = 20)
df_melo = melo(df_pw, melo_iterations = 20)
df_melo
devtools::load_all(".")
norm_melo(df_melo)
devtools::load_all(".")
norm_melo(df_melo)
test(20)
library(n.mEloRatings)
library(n.mEloRatings)
df_pw = loader('/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/example_dataset.csv')
devtools::load_all(".")
df_pw = loader('/Users/Jenny/Library/CloudStorage/OneDrive-UniversityofSurrey/Documents/PhD/3_Preparedness_questionnaire/n.mEloRatings_package/example_dataset.csv')
df_pw
elo_sequence(df_pw)
df_melo = melo(df_pw)
df_melo = melo(df_pw, melo_iterations = 10)
df_melo
devtools::load_all(".")
df_pw
elo_sequence(df_pw)
devtools::load_all(".")
elo_sequence(df_pw)
elo(df_pw)
melo(df_pw, melo_randomisations = 20)
df_melo = melo(df_pw, melo_randomisations = 20)
norm_melo(df_melo)
head(df_pw)
elo(df_pw)
melo(df_pw, melo_randomisations = 500, initial_rating = 0, k = 100)
norm_melo(df_melo)
